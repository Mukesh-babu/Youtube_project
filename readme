Title: YouTube Data Harvesting and Warehousing using SQL, MongoDB and Streamlit

Overview: This project focuses on harvesting and warehousing YouTube data through the YouTube API, utilizing both SQL and MongoDB databases for storage. The data is then presented in a user-friendly interface using Streamlit.

Features: YouTube Data Harvesting:

Utilize the YouTube API to extract data such as video details, comments, and statistics. Data Warehousing:

Implement a dual-database system with SQL and MongoDB. SQL for structured data, storing information like video metadata. MongoDB for unstructured data, housing elements like comments and user interactions. Streamlit Dashboard:

Create an interactive dashboard using Streamlit for easy data exploration. Provide users with the ability to filter and visualize data dynamically. SQL Database Schema:

Define a clear and efficient SQL database schema to organize structured data effectively. MongoDB Data Model:

Design an appropriate data model for MongoDB to handle unstructured data, ensuring scalability and performance. ETL (Extract, Transform, Load):

Develop an ETL process to seamlessly transfer data from the YouTube API to the SQL and MongoDB databases. User Authentication:

Implement a secure user authentication system for accessing the Streamlit dashboard. Data Analysis:

Provide basic data analysis features within the Streamlit dashboard, such as trending videos, popular channels, and user engagement metrics. Documentation:

Create comprehensive documentation with setup instructions, dependencies, and usage guidelines. Technologies Used: YouTube API: For fetching data from YouTube. SQL Database (e.g., MySQL, PostgreSQL): For structured data storage. MongoDB: For unstructured data storage. Streamlit: For building the interactive web dashboard. Getting Started: Installation:

Clone the repository. Install the required dependencies specified in the documentation. Configuration:

Set up API keys for the YouTube API. Configure database connections in the appropriate configuration files. Run the ETL Process:

Execute the ETL process to fetch data from YouTube and populate the SQL and MongoDB databases. Start the Streamlit Dashboard:

Launch the Streamlit application to visualize and explore the harvested YouTube data.
workflow: 
# YouTube Data Harvesting and Warehousing Project

## Overview

This project focuses on harvesting YouTube data and storing it in a data warehouse. The data is collected using the YouTube Data API, processed, and then stored in both SQL and MongoDB databases. A Streamlit web application is built to visualize and interact with the stored data.

## Workflow

1. **YouTube Data Harvesting:**
   - Obtain API keys for the YouTube Data API.
   - Configure API access in `config.py`.
   - Run `harvest_data.py` to fetch YouTube video data based on specified criteria (e.g., keywords, channels).

2. **Data Processing and Warehousing:**
   - Process the raw data to extract relevant information.
   - Store the processed data in both SQL (using MySQL in this example) and MongoDB databases.
   - Update database connection details in `config.py`.

3. **Streamlit Web Application:**
   - Build a Streamlit web application to interact with the stored data.
   - Customize the application according to specific requirements.
   - Run the application using `streamlit run app.py`.

## Project Structure
## Setup
1. Install dependencies using `pip install -r requirements.txt`.
2. Configure API keys and database connections in `config.py`.
3. Execute `harvest_data.py` to fetch YouTube data.
4. Run `process_data.py` to process and store data in databases.
5. Launch the Streamlit app using `streamlit run app.py`.

## Dependencies

- pandas
- requests
- pymongo
- mysql-connector-python
- streamlit

## Note

Ensure that you comply with YouTube API usage policies and guidelines when using API keys for data harvesting.

Feel free to customize the project structure, data processing steps, and database choices based on your specific needs.
